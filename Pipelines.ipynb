{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's make the code cells wider, we've got a big screen for a reason!\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "display(HTML(\"<style>.container { width:85% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_classification, load_iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a fake numeric dataset for the purpose of our examples.\n",
    "\n",
    "We'll do this by using scikit-learn's helpful `make_classification` function.\n",
    "\n",
    "<mark>You can 'unpack' arguments into a function via a dictionary using the `**` notation (see below).</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_classification_dict = {'n_samples': 100000, 'n_features': 50}\n",
    "\n",
    "sample_data = make_classification(**make_classification_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.388069</td>\n",
       "      <td>0.904675</td>\n",
       "      <td>0.765426</td>\n",
       "      <td>0.573775</td>\n",
       "      <td>-0.049434</td>\n",
       "      <td>-1.229598</td>\n",
       "      <td>-1.206298</td>\n",
       "      <td>-0.341459</td>\n",
       "      <td>-0.272875</td>\n",
       "      <td>1.638739</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.045696</td>\n",
       "      <td>2.289716</td>\n",
       "      <td>-0.963053</td>\n",
       "      <td>0.849536</td>\n",
       "      <td>-1.394746</td>\n",
       "      <td>-0.342471</td>\n",
       "      <td>0.013916</td>\n",
       "      <td>0.193423</td>\n",
       "      <td>0.222609</td>\n",
       "      <td>0.287836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.647733</td>\n",
       "      <td>-1.048923</td>\n",
       "      <td>0.650805</td>\n",
       "      <td>-2.164396</td>\n",
       "      <td>-1.373018</td>\n",
       "      <td>-0.055490</td>\n",
       "      <td>0.795744</td>\n",
       "      <td>-1.886932</td>\n",
       "      <td>-1.322406</td>\n",
       "      <td>-0.554796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877677</td>\n",
       "      <td>-0.600360</td>\n",
       "      <td>-1.736006</td>\n",
       "      <td>-0.848874</td>\n",
       "      <td>0.101609</td>\n",
       "      <td>-0.240057</td>\n",
       "      <td>-1.730964</td>\n",
       "      <td>0.005355</td>\n",
       "      <td>0.016654</td>\n",
       "      <td>-1.231224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.196932</td>\n",
       "      <td>-0.566312</td>\n",
       "      <td>1.600226</td>\n",
       "      <td>1.361517</td>\n",
       "      <td>-1.072411</td>\n",
       "      <td>0.059933</td>\n",
       "      <td>1.015081</td>\n",
       "      <td>-1.730238</td>\n",
       "      <td>1.292043</td>\n",
       "      <td>0.521457</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.629627</td>\n",
       "      <td>-0.380323</td>\n",
       "      <td>-1.137140</td>\n",
       "      <td>-0.395493</td>\n",
       "      <td>-0.235472</td>\n",
       "      <td>-0.119447</td>\n",
       "      <td>1.351482</td>\n",
       "      <td>-0.305511</td>\n",
       "      <td>-1.601222</td>\n",
       "      <td>-0.259824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.876862</td>\n",
       "      <td>1.278279</td>\n",
       "      <td>-0.629812</td>\n",
       "      <td>-0.903798</td>\n",
       "      <td>-0.071253</td>\n",
       "      <td>0.631750</td>\n",
       "      <td>1.383593</td>\n",
       "      <td>1.029372</td>\n",
       "      <td>0.591521</td>\n",
       "      <td>0.969519</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365821</td>\n",
       "      <td>-1.380773</td>\n",
       "      <td>-1.178331</td>\n",
       "      <td>1.226804</td>\n",
       "      <td>0.168191</td>\n",
       "      <td>-0.967657</td>\n",
       "      <td>1.785998</td>\n",
       "      <td>0.211960</td>\n",
       "      <td>0.007668</td>\n",
       "      <td>0.602140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.780190</td>\n",
       "      <td>-0.253192</td>\n",
       "      <td>-0.228805</td>\n",
       "      <td>0.440978</td>\n",
       "      <td>-0.437337</td>\n",
       "      <td>1.571321</td>\n",
       "      <td>-0.653941</td>\n",
       "      <td>-1.219301</td>\n",
       "      <td>0.190205</td>\n",
       "      <td>2.064969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194003</td>\n",
       "      <td>-1.717657</td>\n",
       "      <td>0.644678</td>\n",
       "      <td>1.350398</td>\n",
       "      <td>0.405357</td>\n",
       "      <td>-1.283523</td>\n",
       "      <td>-0.196359</td>\n",
       "      <td>-0.688351</td>\n",
       "      <td>0.018954</td>\n",
       "      <td>0.863356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -2.388069  0.904675  0.765426  0.573775 -0.049434 -1.229598 -1.206298   \n",
       "1 -0.647733 -1.048923  0.650805 -2.164396 -1.373018 -0.055490  0.795744   \n",
       "2  1.196932 -0.566312  1.600226  1.361517 -1.072411  0.059933  1.015081   \n",
       "3  0.876862  1.278279 -0.629812 -0.903798 -0.071253  0.631750  1.383593   \n",
       "4 -0.780190 -0.253192 -0.228805  0.440978 -0.437337  1.571321 -0.653941   \n",
       "\n",
       "         7         8         9   ...        40        41        42        43  \\\n",
       "0 -0.341459 -0.272875  1.638739  ... -1.045696  2.289716 -0.963053  0.849536   \n",
       "1 -1.886932 -1.322406 -0.554796  ...  0.877677 -0.600360 -1.736006 -0.848874   \n",
       "2 -1.730238  1.292043  0.521457  ... -0.629627 -0.380323 -1.137140 -0.395493   \n",
       "3  1.029372  0.591521  0.969519  ...  0.365821 -1.380773 -1.178331  1.226804   \n",
       "4 -1.219301  0.190205  2.064969  ... -0.194003 -1.717657  0.644678  1.350398   \n",
       "\n",
       "         44        45        46        47        48        49  \n",
       "0 -1.394746 -0.342471  0.013916  0.193423  0.222609  0.287836  \n",
       "1  0.101609 -0.240057 -1.730964  0.005355  0.016654 -1.231224  \n",
       "2 -0.235472 -0.119447  1.351482 -0.305511 -1.601222 -0.259824  \n",
       "3  0.168191 -0.967657  1.785998  0.211960  0.007668  0.602140  \n",
       "4  0.405357 -1.283523 -0.196359 -0.688351  0.018954  0.863356  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(sample_data[0])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our y: <br>\n",
    "<mark>You can continue code onto the next line with `\\` (see below)</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  0\n",
       "2  0\n",
       "3  0\n",
       "4  0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.Series(sample_data[1])\\\n",
    ".to_frame()\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Pipelines\n",
    "<img style=\"float: center;width:250px;height:150px;\" src=\"images/simple_pipelines.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Prior to pipelines, the best way of organising the steps in your model would be to define separate variables for all steps.\n",
    "\n",
    "\n",
    "Disadvantages\n",
    "* Polluting the namespace\n",
    "* Potential data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() #we instantiate the scaler object\n",
    "X_train_scaled = scaler.fit_transform(X_train) #we fit and transform X into the scaler object using .fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's say we want to apply a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(X_train_scaled, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87248"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, imagine we've got lots of steps, <br>\n",
    "let's say we're applying a `VarianceThreshold()` or a `SelectKBest` <br>\n",
    "We would have to do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest\n",
    "\n",
    "k_best = SelectKBest(k = 'all')\n",
    "X_train_k_best = k_best.fit_transform(X_train, y_train.values.ravel())\n",
    "\n",
    "scaler = StandardScaler() #we instantiate the scaler object\n",
    "X_train_scaled = scaler.fit_transform(X_train_k_best) #we fit and transform X into the scaler object using .fit_transform()\n",
    "\n",
    "var_thres = VarianceThreshold(threshold = 0.0)\n",
    "X_train_var_thresh = var_thres.fit_transform(X_train_scaled)\n",
    "\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(X_train_var_thresh, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to apply the same steps to the test data. <br>\n",
    "But in a way which avoids data leakage. <br>\n",
    "We have to use the _information learned from the train data_ and apply it to our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_k_best = X_test.iloc[:, k_best.get_support(indices=True)]\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test_k_best)\n",
    "\n",
    "X_test_var_thresh = var_thres.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87268"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_var_thresh, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the anova p_values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff9630bacc0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "pd.DataFrame(k_best.pvalues_).sort_values(0, ascending = False).plot(kind = 'bar', title= 'anova p_values', figsize = (16, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above isn't ideal, and doesn't scale very well with large processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What if we want cross-validation with some parameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> The code cell below takes a while to run, so feel free to skip.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': array([1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03,\n",
       "       1.e+04])},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logreg_cv = GridSearchCV(cv = 5, estimator=LogisticRegression(solver = 'lbfgs'),\n",
    "                         param_grid={'C': np.logspace(-4, 4, num = 9)})\n",
    "logreg_cv.fit(X_train_scaled, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "logreg_cv.best_estimator_.score(X_test_scaled, y_test)\n",
    "logreg_cv_y_pred = logreg_cv.predict(X_test_scaled)\n",
    "logreg_cv_y_scores = logreg_cv.predict_proba(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Oh no:</b> We have just commited <b>data leakage.<b>\n",
    "</div>\n",
    "    \n",
    "But... how? <br>\n",
    "We are using values scaled (`StandardScalar`) to the *whole population* in a cross-validation (`GridSearchCV`) with data that is less than the *whole population*. <br>\n",
    "\n",
    "A pipeline could have prevented this, let's convert the example above into one such form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> The code cell below takes a while to run, so feel free to skip.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_reg_pipe = make_pipeline(\n",
    "    SelectKBest(k='all')\n",
    "    ,VarianceThreshold()\n",
    "    ,StandardScaler()\n",
    "    ,LogisticRegression(solver = 'lbfgs'))\n",
    "# Notice the parameter grid syntax has changed to the form: 'estimatorname__parameter'\n",
    "pg = {'logisticregression__C': np.logspace(-4, 4, num = 9)}\n",
    "\n",
    "log_reg = GridSearchCV(cv = 5, estimator = log_reg_pipe, param_grid= pg)\n",
    "log_reg.fit(X_train, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now values are scaled **within each k-fold**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_reg.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can score the pipeline as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_reg.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's apply this pipeline on a new dataset! <br>\n",
    "Let's use the common [iris](https://en.wikipedia.org/wiki/Iris_flower_data_set) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "load_iris()['target'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_X = load_iris()['data']\n",
    "iris_y = load_iris()['target']\n",
    "iris_X_train, iris_X_test, iris_y_train, iris_y_test = train_test_split(iris_X, iris_y, stratify = iris_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we clone the pipeline so we can still use the steps of the previously trained data without interfering with logreg. <br>\n",
    "Think of this as the difference between `df2 = df1` and `df2 = df1.copy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import clone\n",
    "\n",
    "log_reg_iris = clone(log_reg)\n",
    "# Setting the multi_class parameter to 'auto' due to this problem having three classes\n",
    "log_reg_iris.estimator.named_steps['logisticregression'].multi_class = 'auto'\n",
    "# Increasing max_iter so the SGD algorithm has more time to converge\n",
    "log_reg_iris.estimator.named_steps['logisticregression'].max_iter = 1000\n",
    "# Setting values to avoid future warnings\n",
    "log_reg_iris.error_score = 'raise'\n",
    "log_reg_iris.iid = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Then we fit as normal\n",
    "log_reg_iris.fit(iris_X_train, iris_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_reg_iris.score(iris_X_test, iris_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "iris_log_reg_y_pred = log_reg_iris.predict(iris_X_test)\n",
    "iris_log_reg_y_scores = log_reg_iris.predict_proba(iris_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It's as simple as that. <br>\n",
    "\n",
    "[Scikit-Learn Pipeline Guide](https://scikit-learn.org/stable/modules/compose.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Pipelines as objects\n",
    "\n",
    "Pipelines are objects, let's say we want to change the final step to SVM rather than Logistic, this is one way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_pipe = clone(log_reg_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can access the steps in a pipelines two ways: <br>\n",
    "`.named_steps` gives a dictionary. <br>\n",
    "`.steps` gives a series of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svm_pipe.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svm_pipe.steps[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's redefine the last step in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svm_pipe.steps[-1] = ('svc', SVC(random_state=0, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can see now that the pipeline has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svm_pipe.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svm_pg = {\"svc__kernel\": ['rbf']\n",
    "         ,\"svc__C\": [0.1]\n",
    "         ,\"svc__gamma\": [0.001]}\n",
    "\n",
    "svm_iris = GridSearchCV(cv = 3, estimator = svm_pipe, param_grid = svm_pg, iid=False)\n",
    "svm_iris.fit(iris_X_train, iris_y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "svm_iris.score(iris_X_test, iris_y_test)\n",
    "iris_svm_y_pred = svm_iris.predict(iris_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Complex pipelines\n",
    "What happens when we want to apply different pipelines to different columns of our data? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img style=\"float: left;\" src=\"images/complex_pipelines.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "# Categorical Columns\n",
    "fill_to_zero = [0]\n",
    "# Define steps in pipeline\n",
    "cat_pipe = make_pipeline(SimpleImputer(strategy=\"constant\", fill_value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll make a new pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "preprocessing = make_column_transformer((cat_pipe, fill_to_zero), remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The below is a lot to take in, let's take it step by step. <br>\n",
    "1. First we use C_base and C_list to create a list of potential `C` that we will grid search over. <br>\n",
    "`C` is the inverse of the regularisation strength. <br>\n",
    "1. Then the pipeline begins, we run the data through the \"preprocessing\" pipeline defined above. <br>\n",
    "1. Then we impute with the median. <br>\n",
    "1. Then we run the data through a Random Forest to see what the feature importances are. <br>\n",
    "1. Only the top 20 features progress to the next stage. This can be useful when dealing with hundreds of features. <br>\n",
    "1. Next all values are scaled such that the mean is 0 and the variance is 1 (unit variance). <br>\n",
    "1. Finally the data is run through `LogisticRegressionCV`, think of this as `LogisticRegression` combined with `GridSearchCV`, this approach can be quicker due to a backend phenonmenon called *warm starting* (using the previous solution as an initialization for the following fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "C_base = 4\n",
    "C_list = np.logspace(-C_base, +C_base, num=(2 * C_base) + 1)\n",
    "lr_pipe = make_pipeline(\n",
    "    preprocessing,\n",
    "    SimpleImputer(strategy=\"median\"),\n",
    "    SelectFromModel(RandomForestClassifier(random_state=0, n_estimators=20)),\n",
    "    StandardScaler(),  # Adding in a standard scaling step, relative to the dtc section.\n",
    "    LogisticRegressionCV(random_state=0, solver=\"saga\", cv=5, penalty=\"l1\", class_weight=\"balanced\", scoring=\"accuracy\", max_iter=10000, n_jobs=-1, multi_class='ovr')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_pipe.fit(iris_X_train, iris_y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_pipe.score(iris_X_test, iris_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**A quick note on feature names**\n",
    "So we've applied a lot of steps here.\n",
    "But interpretability is important, so it would be good to know what features went into the final step.\n",
    "\n",
    "A simple `pipeline.get_feature_names()` is not yet implemented in scikit-learn. <br>\n",
    "But the pipeline above is not too long, so we can do this manually.\n",
    "\n",
    "Below is an example that works for a pipeline with many features, I'll talk through how each step works, but first here's the example pipeline.\n",
    "\n",
    "**Preprocessing pipeline**\n",
    "\n",
    "```\n",
    "# Categorical Columns\n",
    "categoric_variables = [\"CITY\"]\n",
    "# Define steps in pipeline\n",
    "cat_pipe = make_pipeline(SimpleImputer(strategy=\"constant\", fill_value=\"0\"), OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "\n",
    "\n",
    "# Column transformer to deal with the categoric variable\n",
    "preprocessing = make_column_transformer((cat_pipe, categoric_variables), remainder=\"passthrough\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Random Forest Pipeline**\n",
    "\n",
    "```\n",
    "# Random Forest\n",
    "`rf_pipe = make_pipeline(preprocessing\n",
    "                        ,SimpleImputer(strategy=\"median\")\n",
    "                        ,VarianceThreshold()\n",
    "                        ,RandomForestClassifier(random_state=0))\n",
    "rf_parameters = {\"randomforestclassifier__class_weight\": [\"balanced\"]\n",
    "                ,\"randomforestclassifier__n_estimators\": [20, 50, 70, 100]}\n",
    "                \n",
    "rf_gs = GridSearchCV(rf_pipe\n",
    "                    ,param_grid = rf_parameters\n",
    "                    ,scoring=scoring_list\n",
    "                    ,refit=\"roc_auc\"\n",
    "                    ,return_train_score=False\n",
    "                    ,cv=5)\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "feature_names = list(pipe.named_steps.columntransformer.named_transformers_.pipeline.named_steps.onehotencoder.get_feature_name(categoric_variables)) +\n",
    "[column for column in X.columns if column not in categoric_variables]\n",
    "\n",
    "feature_names = [list(feature_names)[i] for i in pipe.named_steps.variancethreshold.get_support(indices=True)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Useful Packages\n",
    "Below are some useful packages for machine learning that help abstract boilerplate code. <br>\n",
    "1. eli5 - basic ML interpretability. \n",
    "1. scikitplot - matplotlib plots directly from model artefacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "eli5.explain_weights_sklearn(log_reg_iris.best_estimator_.named_steps.logisticregression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_confusion_matrix, plot_roc, plot_cumulative_gain\n",
    "plot_confusion_matrix(iris_y_test, iris_log_reg_y_pred)\n",
    "plot_roc(iris_y_test, iris_log_reg_y_scores, title = 'my fake roc curve ¯\\_(-_-)_/¯', plot_micro = False, plot_macro = False)\n",
    "plot_cumulative_gain(y_test, logreg_cv_y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = plt.axes()\n",
    "\n",
    "plot_roc(iris_y_test, iris_log_reg_y_scores, ax = test, plot_macro = False, plot_micro = False)\n",
    "plot_roc(y_test, logreg_cv_y_scores, ax = test, plot_macro = False, plot_micro = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "759px",
    "left": "40px",
    "top": "110px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
